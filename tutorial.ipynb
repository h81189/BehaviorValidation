{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Behavioral Phenotyping with 3D Skeletal Pose\n",
    "Joshua Wu\n",
    "Duke University Biomedical Engineering\n",
    "Timothy Dunn Lab\n",
    "\n",
    "25 July, 2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neurodegenerative diseases (like Parkinson's) are characterized by a wide variety of behavioral defects or movement deficits. However, behavior and movement have historically been difficult to quantify and measure. Recent developments in hardware and machine learning have enabled more objective behavioral metrics by providing continuous 3D measurements of naturalistic animal behavior through multi-view videos. These new modalities of data offer a means by which we can comprehensively characterize behavioral phenotypes of neural (dys)-function. We present `dappy` to establish an open-source API with easy access to machine learning methods for the analysis of 3D pose sequences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a Python version of [CAPTURE (Marshall, 2020)](https://www.cell.com/neuron/fulltext/S0896-6273(20)30894-1?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0896627320308941%3Fshowall%3Dtrue), which was based on earlier work [MotionMapper (Berman, 2014)](https://royalsocietypublishing.org/doi/full/10.1098/rsif.2014.0672) for the analysis of behavioral data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get this notebook to run, please download the [demo dataset](https://duke.box.com/v/demo-mouse-poses) into the `/dappy/tutorials/data/` directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dappy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/hpc/group/tdunn/hk276/tango/tutorials/tutorial.ipynb Cell 6\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226463632d6c6f67696e2e6f69742e64756b652e656475222c2275736572223a22686b323736227d/hpc/group/tdunn/hk276/tango/tutorials/tutorial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdappy\u001b[39;00m \u001b[39mimport\u001b[39;00m read, write\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226463632d6c6f67696e2e6f69742e64756b652e656475222c2275736572223a22686b323736227d/hpc/group/tdunn/hk276/tango/tutorials/tutorial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdappy\u001b[39;00m \u001b[39mimport\u001b[39;00m visualization \u001b[39mas\u001b[39;00m vis\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a226463632d6c6f67696e2e6f69742e64756b652e656475222c2275736572223a22686b323736227d/hpc/group/tdunn/hk276/tango/tutorials/tutorial.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dappy'"
     ]
    }
   ],
   "source": [
    "from dappy import read, write\n",
    "from dappy import visualization as vis\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import Video\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pose predictions, keypoint connectivity information, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_key = \"tutorial\"\n",
    "config = read.config(\"../configs/\" + analysis_key + \".yaml\")\n",
    "\n",
    "pose, ids = read.pose_h5(config[\"data_path\"] + \"demo_mouse.h5\")\n",
    "\n",
    "connectivity = read.connectivity(\n",
    "    path=config[\"skeleton_path\"], skeleton_name=config[\"skeleton_name\"]\n",
    ")\n",
    "\n",
    "meta, meta_by_frame = read.meta(config[\"data_path\"] + \"demo_meta.csv\", id=ids)\n",
    "\n",
    "Path(config[\"out_path\"]).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pose` shape (# frames x # keypoints x 3 coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pose shape (# frames x # keypoints x 3 coordinates): \")\n",
    "print(pose.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`meta` contains categorical information on recording sessions in `pose`. Here, we have loaded in two sessions. Each frame of the `pose` has a session id label in `ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta)\n",
    "print(\"\\n\" + str(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`connectivity` contains key information indicating keypoint labels, connectivity, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"keypoint labels\")\n",
    "print(connectivity.joint_names)\n",
    "print(\"\\n Keypoint connections\")\n",
    "print(connectivity.links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot 150 frames from each session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.pose3D_arena(\n",
    "    pose,\n",
    "    connectivity,\n",
    "    frames=[1000, 500000],\n",
    "    N_FRAMES=150,\n",
    "    dpi=100,\n",
    "    VID_NAME=\"raw.mp4\",\n",
    "    SAVE_ROOT=config[\"out_path\"],\n",
    ")\n",
    "\n",
    "Video(config[\"out_path\"] + \"vis_raw.mp4\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeletons across sessions may not be aligned worldviews. The following code will estimate the floor plane for each session, and rotate to the x-y plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dappy import preprocess\n",
    "\n",
    "pose_aligned = preprocess.align_floor_by_id(pose=pose, ids=ids, foot_id=12, head_id=0)\n",
    "\n",
    "vis.pose3D_arena(\n",
    "    pose_aligned,\n",
    "    connectivity,\n",
    "    frames=[1000, 500000],\n",
    "    N_FRAMES=150,\n",
    "    dpi=100,\n",
    "    VID_NAME=\"aligned.mp4\",\n",
    "    SAVE_ROOT=config[\"out_path\"],\n",
    ")\n",
    "\n",
    "Video(config[\"out_path\"] + \"vis_aligned.mp4\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following code to save the new aligned poses for easy access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dappy import write\n",
    "\n",
    "# write.pose_h5(pose_aligned, ids, config[\"data_path\"] + \"pose_aligned.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, we would like to prevent divergence of behavioral representations due to global position. Thus, we will generate an egocentric representation of pose for downstream feature calculation. \n",
    "\n",
    "Here, we center the mid-spine to $(0,0,0)$, and rotate the front-spine to the $x+$ direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide the mid-spine and the mid-spine -> front-spine indices.\n",
    "pose = preprocess.rotate_spine(preprocess.center_spine(pose_aligned, keypt_idx=4), keypt_idx=[4, 3])\n",
    "\n",
    "vis.skeleton_vid3D(\n",
    "    pose,\n",
    "    connectivity,\n",
    "    frames=[50000],\n",
    "    N_FRAMES=150,\n",
    "    dpi=100,\n",
    "    VID_NAME=\"centered.mp4\",\n",
    "    SAVE_ROOT=config[\"out_path\"],\n",
    ")\n",
    "\n",
    "Video(config[\"out_path\"] + \"vis_centered.mp4\", width=600, height=600)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this package, we provide functionality for easily calculating features of interest. \n",
    "\n",
    "Using this centered and spine-locked pose transformation, we can calculate relative velocities of all keypoints. We leave out the mid spine since it is centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dappy import features\n",
    "\n",
    "# Getting relative velocities\n",
    "rel_vel, rel_vel_labels = features.get_velocities(\n",
    "    pose,\n",
    "    ids,\n",
    "    connectivity.joint_names,\n",
    "    joints=np.delete(np.arange(18), 4),\n",
    "    widths=[5, 11, 51],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also calculate joint angles of interest as specified in `skeletons.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(connectivity.angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating joint angles\n",
    "angles, angle_labels = features.get_angles(pose, connectivity.angles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These velocity and angle calculations are just for demonstration, we will not use velocities or angles for the analysis in this tutorial.\n",
    "\n",
    "We will just rearrange egocentric x, y, z coordinates of each keypoint into its own set of features. This code does not calculate anything - it just reshapes the pose and generates labels for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape pose to get egocentric pose features\n",
    "features, labels = features.get_ego_pose(pose, connectivity.joint_names)\n",
    "\n",
    "# Clear some memory\n",
    "del angles, rel_vel, angel_labels, rel_vel_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write features to or read features from `.h5` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write\n",
    "# write.features_h5(features, labels, path=config[\"out_path\"] + \"postural_feats.h5\")\n",
    "\n",
    "# Read\n",
    "# features, labels = read.features_h5(path=config[\"out_path\"] + \"postural_feats.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time for principal component analysis (PCA). PCA is a dimensionality reduction technique which generates orthogonal axes of high variance upon which to project our data. There are many implementations of PCA, but we will use Facebook's Fast Randomized PCA package (`fbpca`), which is significantly faster than most other implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "pc_feats, pc_labels = features.pca(\n",
    "    features, labels, categories=[\"ego_euc\"], n_pcs=5, method=\"fbpca\"\n",
    ")\n",
    "print(\"PCA time: \" + str(time.time() - t))\n",
    "\n",
    "del features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although velocities are calculated over rolling windows, the featurization we have so far still lacks the ability to capture complex temporal signals.\n",
    "\n",
    "To address this, we can leverage the frequency domain through a Morlet wavelet transformation.\n",
    "\n",
    "Let's see first what a Morlet wavelet looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "M = 100\n",
    "w0 = 5\n",
    "s = w0*90/(2*np.pi*25)\n",
    "morlet_wavelet = signal.morlet2(M, s, w0)\n",
    "plt.plot(morlet_wavelet.imag, label='Imaginary')\n",
    "plt.plot(morlet_wavelet.real, label='Real')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlet_feats, wlet_labels = features.wavelet(\n",
    "    pc_feats, pc_labels, ids, sample_freq=90, freq=np.linspace(1, 25, 25), w0=5\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use PCA to reduce the dimensions of the new wavelet features, and consolidate with the previous PC scores. Each frame is now associated with a vector of features corresponding to the PC scores of egocentric keypoint coordinates and local frequency information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on wavelet features\n",
    "pc_wlet, pc_wlet_labels = features.pca(\n",
    "    wlet_feats,\n",
    "    wlet_labels,\n",
    "    categories=[\"wlet_ego_euc\"],\n",
    "    n_pcs=5,\n",
    "    method=\"fbpca\",\n",
    ")\n",
    "\n",
    "del wlet_feats, wlet_labels\n",
    "pc_feats = np.hstack((pc_feats, pc_wlet))\n",
    "pc_labels += pc_wlet_labels\n",
    "del pc_wlet, pc_wlet_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save full PC features to file\n",
    "# write.features_h5(\n",
    "#     pc_feats, pc_labels, path=\"\".join([config[\"out_path\"], \"pca_feats.h5\"])\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encapsulate all relevant data to store in a data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dappy import DataStruct as ds\n",
    "\n",
    "data_obj = ds.DataStruct(\n",
    "    pose=pose,\n",
    "    id=ids,\n",
    "    meta=meta,\n",
    "    meta_by_frame=meta_by_frame,\n",
    "    connectivity=connectivity,\n",
    ")\n",
    "\n",
    "data_obj.features = pc_feats\n",
    "# Downsampling data, appears to be necessary in order to \n",
    "# discover granular structure in embedding\n",
    "data_obj = data_obj[:: config[\"downsample\"], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using t-SNE, frames are projected onto a 2D embedding for clustering and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dappy.embed import Embed\n",
    "# Embedding using fitsne\n",
    "embedder = Embed(\n",
    "    embed_method=config[\"single_embed\"][\"method\"],\n",
    "    perplexity=config[\"single_embed\"][\"perplexity\"],\n",
    "    lr=config[\"single_embed\"][\"lr\"],\n",
    ")\n",
    "data_obj.embed_vals = embedder.embed(data_obj.features, save_self=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of the 2D embedding is smoothed with a Gaussian, and segmented by the watershed algorithm to determine cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dappy.embed import Watershed\n",
    "# Watershed clustering\n",
    "data_obj.ws = Watershed(\n",
    "    sigma=config[\"single_embed\"][\"sigma\"], max_clip=1, log_out=True, pad_factor=0.05\n",
    ")\n",
    "data_obj.data.loc[:, \"Cluster\"] = data_obj.ws.fit_predict(data=data_obj.embed_vals)\n",
    "\n",
    "# Plot density\n",
    "vis.density(\n",
    "    data_obj.ws.density,\n",
    "    data_obj.ws.borders,\n",
    "    filepath=config[\"out_path\"] + \"/density.png\",\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the embedding, we can visualize the density of each animal separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.density_cat(\n",
    "    data=data_obj,\n",
    "    column=\"id\",\n",
    "    watershed=data_obj.ws,\n",
    "    n_col=4,\n",
    "    filepath=config[\"out_path\"] + \"/density_id.png\",\n",
    "    show=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('capture')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "385c92b5077163337880ed1116fdafd989b4fe218c24e087321f17c8274ea8b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
